{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8a6cadf",
   "metadata": {},
   "source": [
    "# Multi-Agent Demo: LangGraph + Recursive Companion\n",
    "\n",
    "## Simple, Modular Design\n",
    "\n",
    "### üéØ The RC Difference: Agents are Just Callables\n",
    "\n",
    "Unlike LangChain's complex abstractions (Chains, Agents, Tools, Memory), Recursive Companion agents are **simple Python callables**:\n",
    "\n",
    "```python\n",
    "# Create an agent\n",
    "agent = MarketingCompanion()\n",
    "\n",
    "# Use it anywhere - no special interfaces needed!\n",
    "result = agent(\"Why did sales drop?\")           # Direct call\n",
    "result = agent.loop(\"Why did sales drop?\")      # Explicit method\n",
    "node = RunnableLambda(agent)                    # LangGraph integration\n",
    "```\n",
    "\n",
    "No abstract base classes. No framework lock-in. Just modular components that work everywhere.\n",
    "\n",
    "### What You'll See in This Demo:\n",
    "\n",
    "This notebook demonstrates how **LangGraph** (workflow orchestration) and **Recursive Companion** (thinking transparency) work together to provide complete observability in multi-agent systems.\n",
    "\n",
    "1. **LangGraph's Strengths**: \n",
    "   - Parallel agent execution\n",
    "   - Clean state management  \n",
    "   - Workflow debugging with `print_mode=\"debug\"`\n",
    "   \n",
    "2. **RC's Addition**:\n",
    "   - Full critique/revision history for each agent\n",
    "   - Convergence metrics and reasoning evolution\n",
    "   - Deep introspection unavailable through orchestration alone\n",
    "\n",
    "### Key Insight:\n",
    "LangGraph tells you **what** happened in your workflow. Recursive Companion shows you **why** each agent reached its conclusions. Together, you get complete system understanding.\n",
    "\n",
    "See [LangGraph Comparison](https://github.com/hankbesser/recursive-companion/blob/main/docs/LangGraph_Compliment.md) for detailed analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae7758e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPDX-License-Identifier: MIT\n",
    "#\n",
    "# Copyright (c) [2025] [Henry Besser]\n",
    "#\n",
    "# This software is licensed under the MIT License.\n",
    "# See the LICENSE file in the project root for the full license text.\n",
    "\n",
    "# demos/multi_agent_langgraph_demo.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30763141",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffde8e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "api_key_status = \"Loaded\" if os.getenv(\"OPENAI_API_KEY\") else \"NOT FOUND - Check your .env file and environment.\"\n",
    "print(f\"OpenAI API Key status: {api_key_status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec4f753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from IPython.display import Image, display, Markdown\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langgraph.graph import StateGraph\n",
    "from typing import TypedDict\n",
    "from recursive_companion.base import MarketingCompanion, BugTriageCompanion, StrategyCompanion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb86558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# different models for differents domains\n",
    "llm_fast  = \"gpt-4o-mini\"\n",
    "llm_deep  = \"gpt-4.1-mini\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7e0931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the agents\n",
    "# tip read the doctring by hovering over the class\n",
    "mkt   = MarketingCompanion(llm=llm_fast, temperature=0.8, max_loops=3, similarity_threshold=0.96)\n",
    "eng   = BugTriageCompanion(llm=llm_deep, temperature=0.3)\n",
    "plan = StrategyCompanion(llm=llm_fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463b02e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RC companions work as drop-in LangGraph nodes!\n",
    "# The __call__ method makes them compatible with RunnableLambda\n",
    "# No special integration needed - they just work together\n",
    "\n",
    "mkt_node  = RunnableLambda(mkt)          # Marketing companion ‚Üí LangGraph node\n",
    "eng_node  = RunnableLambda(eng)          # Engineering companion ‚Üí LangGraph node\n",
    "\n",
    "# Now these nodes have BOTH:\n",
    "# - LangGraph's orchestration capabilities (streaming, retries, etc.)\n",
    "# - RC's thinking transparency (critique/revision history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cb39cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge-lambda joins text views into one string\n",
    "# note: LangGraph passes the entire upstream-state dict to a node.\n",
    "# with out this function, two upstream nodes are piped straight into strategy, \n",
    "# so plan_node will receive a Python dict like {\"engineering\": \"...\", \"marketing\": \"...\"}.\n",
    "# That's fine if your StrategyCompanion prompt expects that JSON blob, \n",
    "# but most of the time you'll want to concatenate the two strings first.\n",
    "\n",
    "merge_node = RunnableLambda(\n",
    "    lambda d: f\"### Marketing\\n{d['marketing']}\\n\\n### Engineering\\n{d['engineering']}\"\n",
    ")\n",
    "plan_node  = RunnableLambda(plan)\n",
    "\n",
    "# Define the state schema for LangGraph\n",
    "class GraphState(TypedDict):\n",
    "    input: str\n",
    "    marketing: str\n",
    "    engineering: str\n",
    "    merged: str\n",
    "    final_plan: str\n",
    "\n",
    "# Inline LangGraph example (fan-in)\n",
    "# No extra prompts, no schema gymnastics: simply passing text between the callables the classes already expose.\n",
    "graph = StateGraph(GraphState)\n",
    "graph.add_node(\"marketing_agent\",    lambda state: {\"marketing\": mkt_node.invoke(state[\"input\"])})\n",
    "graph.add_node(\"engineering_agent\",  lambda state: {\"engineering\": eng_node.invoke(state[\"input\"])})\n",
    "graph.add_node(\"merge_agent\",        lambda state: {\"merged\": merge_node.invoke(state)})\n",
    "graph.add_node(\"strategy_agent\",     lambda state: {\"final_plan\": plan_node.invoke(state[\"merged\"])})\n",
    "\n",
    "graph.add_edge(\"marketing_agent\", \"merge_agent\")\n",
    "graph.add_edge(\"engineering_agent\", \"merge_agent\")\n",
    "graph.add_edge(\"merge_agent\", \"strategy_agent\")\n",
    "\n",
    "graph.add_edge(\"__start__\", \"marketing_agent\")\n",
    "graph.add_edge(\"__start__\", \"engineering_agent\")\n",
    "graph.set_finish_point(\"strategy_agent\")\n",
    "workflow = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e326c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the graph\n",
    "display(Image(workflow.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258ed155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture the debug stream output\n",
    "debug_chunks = []\n",
    "\n",
    "for chunk in workflow.stream(\n",
    "    {\"input\": \"App ratings fell to 3.2‚òÖ and uploads crash on iOS 17.2. Diagnose & propose next steps.\"},\n",
    "    print_mode=\"debug\"\n",
    "):\n",
    "    debug_chunks.append(chunk)  # Save each chunk\n",
    "    print(chunk)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hcf40dbbqpc",
   "metadata": {},
   "source": [
    "### üéØ Extracting Just the Final Strategy Result\n",
    "\n",
    "Now let's extract ONLY the final strategy agent's conclusion from all that debug output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zkeq9mloaod",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the final strategy result from the debug stream\n",
    "# The strategy agent's result is in the last task_result with name='strategy_agent'\n",
    "strategy_result = None\n",
    "for chunk in reversed(debug_chunks):  # Search from the end\n",
    "    if (chunk.get('type') == 'task_result' and \n",
    "        chunk.get('payload', {}).get('name') == 'strategy_agent'):\n",
    "        # The result is in payload.result[0][1] \n",
    "        # (list of tuples where first item is key, second is value)\n",
    "        strategy_result = chunk['payload']['result'][0][1]\n",
    "        break\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã EXTRACTED STRATEGY RESULT:\")\n",
    "print(\"=\"*80)\n",
    "if strategy_result:\n",
    "    print(strategy_result[:500] + \"...\" if len(strategy_result) > 500 else strategy_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ycuc03wvhij",
   "metadata": {},
   "source": [
    "### üìä Capturing LangGraph's Debug Stream\n",
    "\n",
    "First, we need to capture all the debug output to extract results later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d418f76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === OBSERVABILITY COMPARISON ===\n",
    "print(\"=\" * 80)\n",
    "print(\"üîç COMPLEMENTARY OBSERVABILITY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n‚úÖ LangGraph's Workflow Debugging (print_mode='debug'):\")\n",
    "print(\"  ‚Ä¢ Task scheduling and execution order\")\n",
    "print(\"  ‚Ä¢ Node inputs/outputs and state transitions\")  \n",
    "print(\"  ‚Ä¢ Parallel execution timing\")\n",
    "print(\"  ‚Ä¢ Error handling and retries\")\n",
    "print(\"  ‚Üí Perfect for debugging orchestration issues!\")\n",
    "\n",
    "print(\"\\n‚úÖ Recursive Companion's Thinking Transparency:\")\n",
    "print(\"  ‚Ä¢ Draft ‚Üí Critique ‚Üí Revision cycles\")\n",
    "print(\"  ‚Ä¢ Similarity scores and convergence patterns\")\n",
    "print(\"  ‚Ä¢ Complete reasoning audit trail\")\n",
    "print(\"  ‚Ä¢ Why agents reached specific conclusions\")\n",
    "print(\"  ‚Üí Essential for understanding agent reasoning!\")\n",
    "\n",
    "print(\"\\nüéØ Better Together:\")\n",
    "print(\"  ‚Ä¢ LangGraph: See the workflow execution flow\")\n",
    "print(\"  ‚Ä¢ RC: Understand the thinking behind decisions\")\n",
    "print(\"  ‚Ä¢ Zero integration overhead - RC companions are drop-in nodes\")\n",
    "print(\"\\nüëâ Scroll down to see both types of observability in action!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mifacqx20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all agent results from the debug stream\n",
    "agent_results = {}\n",
    "\n",
    "for chunk in debug_chunks:\n",
    "    if chunk.get('type') == 'task_result' and chunk.get('payload'):\n",
    "        agent_name = chunk['payload'].get('name', '').replace('_agent', '')\n",
    "        if chunk['payload'].get('result'):\n",
    "            # Result is in format: [(key, value)]\n",
    "            result_value = chunk['payload']['result'][0][1]\n",
    "            agent_results[agent_name] = result_value\n",
    "\n",
    "print(\"üìä ALL AGENT RESULTS FROM DEBUG STREAM:\")\n",
    "print(\"=\"*60)\n",
    "for agent, result in agent_results.items():\n",
    "    print(f\"\\n{agent.upper()}:\")\n",
    "    print(result[:200] + \"...\" if len(result) > 200 else result)\n",
    "    print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0l1oxz3wnni",
   "metadata": {},
   "source": [
    "### üìà Extracting ALL Agent Results \n",
    "\n",
    "Want to see what EVERY agent said? Let's extract all results from the debug stream:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yc1a94xwb7g",
   "metadata": {},
   "source": [
    "## ü§î Notice How Complex Extracting Results from LangGraph Is?\n",
    "\n",
    "### LangGraph Debug Stream Extraction (Complex!)\n",
    "- Need to capture all chunks in a list\n",
    "- Parse nested dictionary structure (`chunk['payload']['result'][0][1]`)\n",
    "- Search through chunks to find the right `task_result`\n",
    "- No access to iteration details or thinking process\n",
    "\n",
    "### RC Access Pattern (Simple!)\n",
    "```python\n",
    "# With RC, everything is immediately accessible:\n",
    "mkt.run_log           # Full iteration history\n",
    "mkt.run_log[-1]       # Last iteration details\n",
    "mkt.transcript_as_markdown()  # Formatted thinking process\n",
    "\n",
    "# No parsing, no searching, no nested dictionaries!\n",
    "```\n",
    "\n",
    "This complexity difference becomes even more pronounced when you need to debug WHY an agent made a decision..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abe8583",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = workflow.invoke(\n",
    "    {\"input\": \"App ratings fell to 3.2‚òÖ and uploads crash on iOS 17.2. Diagnose & propose next steps.\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2831002",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = result.get(\"final_plan\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa5c4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== FINAL PLAN ===\\n\")\n",
    "display(Markdown(final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d89648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === After LangGraph workflow completes ===\n",
    "print(\"\\nüîç DEEP INTROSPECTION - What LangGraph CAN'T normally show you:\\n\")\n",
    "# Show iteration counts\n",
    "print(f\"Marketing iterations: {len(mkt.run_log)}\")\n",
    "print(f\"Engineering iterations: {len(eng.run_log)}\")\n",
    "print(f\"Strategy iterations: {len(plan.run_log)}\")\n",
    "# Show why each converged\n",
    "print(\"\\nüìä CONVERGENCE ANALYSIS:\")\n",
    "for name, agent in [(\"Marketing\", mkt), (\"Engineering\", eng), (\"Strategy\", plan)]:\n",
    "    if len(agent.run_log) < agent.max_loops:\n",
    "        print(f\"{name}: Converged early (quality threshold reached)\")\n",
    "    else:\n",
    "        print(f\"{name}: Used all {agent.max_loops} iterations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f670d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show full thinking process - BEAUTIFULLY FORMATTED!\n",
    "# No parsing, no JSON manipulation - just readable markdown\n",
    "print(\"\\nüß† MARKETING THINKING PROCESS:\")\n",
    "display(Markdown(mkt.transcript_as_markdown()))  # Ready for reports, logs, or UI!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tahbueqr6ds",
   "metadata": {},
   "source": [
    "## üìù Beautiful Output with Zero Parsing!\n",
    "\n",
    "Notice how RC's `transcript_as_markdown()` gives you perfectly formatted output:\n",
    "- **No JSON parsing needed** (unlike LangGraph's nested dictionaries)\n",
    "- **Ready for reports, logs, or UI display**\n",
    "- **Each iteration clearly separated** with Draft ‚Üí Critique ‚Üí Revision\n",
    "- **Just one method call** vs complex chunk extraction\n",
    "\n",
    "Compare this to extracting from LangGraph's debug stream above - night and day!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518a7737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engineering thinking process - each iteration clearly separated\n",
    "print(\"\\nüîß ENGINEERING THINKING PROCESS:\")\n",
    "display(Markdown(eng.transcript_as_markdown()))  # Draft ‚Üí Critique ‚Üí Revision for each loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f46b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy synthesis - complete thinking audit trail\n",
    "print(\"\\nüéØ STRATEGY SYNTHESIS PROCESS:\")\n",
    "display(Markdown(plan.transcript_as_markdown()))  # One method = full thinking history!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515e6201",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recursive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
