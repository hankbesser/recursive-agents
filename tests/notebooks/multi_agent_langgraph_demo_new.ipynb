{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ae7758e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPDX-License-Identifier: MIT\n",
    "#\n",
    "# Copyright (c) [2025] [Henry Besser]\n",
    "#\n",
    "# This software is licensed under the MIT License.\n",
    "# See the LICENSE file in the project root for the full license text.\n",
    "\n",
    "# demos/multi_agent_langgraph_demo.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d650cc3-be40-4b49-8a7e-6853595fbe57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hbesse13/miniconda3/envs/phase/lib/python3.14/site-packages/langchain_core/_api/deprecation.py:26: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'StructuredPrompt' from 'langchain_core.prompts' (/home/hbesse13/miniconda3/envs/phase/lib/python3.14/site-packages/langchain_core/prompts/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StructuredPrompt\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mOutputSchema\u001b[39;00m(BaseModel):\n\u001b[32m      5\u001b[39m     name: \u001b[38;5;28mstr\u001b[39m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'StructuredPrompt' from 'langchain_core.prompts' (/home/hbesse13/miniconda3/envs/phase/lib/python3.14/site-packages/langchain_core/prompts/__init__.py)"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import StructuredPrompt\n",
    "\n",
    "\n",
    "class OutputSchema(BaseModel):\n",
    "    name: str\n",
    "    value: int\n",
    "\n",
    "\n",
    "template = StructuredPrompt(\n",
    "    [\n",
    "        (\"human\", \"Hello, how are you?\"),\n",
    "        (\"ai\", \"I'm doing well, thanks!\"),\n",
    "        (\"human\", \"That's good to hear.\"),\n",
    "    ],\n",
    "    OutputSchema,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a6cadf",
   "metadata": {},
   "source": [
    "# Multi-Agent Demo: LangGraph + Recursive Agents\n",
    "\n",
    "## Combining Workflow Orchestration with Agent Introspection\n",
    "\n",
    "This notebook demonstrates how Recursive Agents agents integrate with LangGraph workflows. Since RA agents are callables, they work seamlessly as LangGraph nodes while preserving their thinking transparency.\n",
    "\n",
    "### What This Demo Shows:\n",
    "\n",
    "1. **Integration**: How RA agents become LangGraph nodes\n",
    "2. **Workflow Execution**: Running agents through LangGraph's orchestration\n",
    "3. **Debug Visibility**: What LangGraph's debug stream provides\n",
    "4. **Thinking History**: How RA's introspection remains accessible\n",
    "\n",
    "The key insight: you get both workflow management and agent reasoning visibility without choosing between them.\n",
    "\n",
    "For detailed analysis, see [`docs/LangGraph_RA_comp.md`](../docs/LangGraph_RA_comp.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30763141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffde8e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key status: Loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "api_key_status = \"Loaded\" if os.getenv(\"OPENAI_API_KEY\") else \"OPENAI NOT FOUND - Check your .env file and environment.\"\n",
    "print(f\"OpenAI API Key status: {api_key_status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca42608c-7de2-4d9f-af2b-9f0a6ec58f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anthropic API Key status: Loaded\n"
     ]
    }
   ],
   "source": [
    "api_key_status = \"Loaded\" if os.getenv(\"ANTHROPIC_API_KEY\") else \"ANTHROPIC NOT FOUND - Check your .env file and environment.\"\n",
    "print(f\"Anthropic API Key status: {api_key_status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ec4f753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from IPython.display import Image, display, Markdown\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langgraph.graph import StateGraph\n",
    "from typing import TypedDict\n",
    "from recursive_agents.base import MarketingCompanion, BugTriageCompanion, StrategyCompanion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e08790d1-9b3f-4850-b93f-cca7fb4c3078",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "llm = ChatAnthropic(\n",
    "    model=\"claude-3-5-haiku-latest\",\n",
    "    temperature=0,\n",
    "    max_tokens=1024,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bd27b05-2472-4f87-a9e8-933787557f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'adore la programmation.\", additional_kwargs={}, response_metadata={'id': 'msg_01EopUfGdxb8SNBuqjFu65CE', 'model': 'claude-3-5-haiku-20241022', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation': {'ephemeral_1h_input_tokens': 0, 'ephemeral_5m_input_tokens': 0}, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 29, 'output_tokens': 11, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-3-5-haiku-20241022', 'model_provider': 'anthropic'}, id='lc_run--eceb0aa6-8b0d-4cce-90cc-8e40ec728443-0', usage_metadata={'input_tokens': 29, 'output_tokens': 11, 'total_tokens': 40, 'input_token_details': {'cache_read': 0, 'cache_creation': 0, 'ephemeral_5m_input_tokens': 0, 'ephemeral_1h_input_tokens': 0}})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1687e818-8251-410a-b677-049fe78d8791",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'StructuredPrompt' from 'langchain_core.prompts' (/home/hbesse13/miniconda3/envs/phase/lib/python3.14/site-packages/langchain_core/prompts/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StructuredPrompt\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mOutputSchema\u001b[39;00m(BaseModel):\n\u001b[32m      5\u001b[39m     name: \u001b[38;5;28mstr\u001b[39m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'StructuredPrompt' from 'langchain_core.prompts' (/home/hbesse13/miniconda3/envs/phase/lib/python3.14/site-packages/langchain_core/prompts/__init__.py)"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import StructuredPrompt\n",
    "\n",
    "\n",
    "class OutputSchema(BaseModel):\n",
    "    name: str\n",
    "    value: int\n",
    "\n",
    "\n",
    "template = StructuredPrompt(\n",
    "    [\n",
    "        (\"human\", \"Hello, how are you?\"),\n",
    "        (\"ai\", \"I'm doing well, thanks!\"),\n",
    "        (\"human\", \"That's good to hear.\"),\n",
    "    ],\n",
    "    OutputSchema,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6a9ce6-bf88-4031-a78d-ab9cabad766c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_fast = \"gpt-4o-mini\"\n",
    "llm_deep = \"gpt-4o\"\n",
    "\n",
    "mkt = MarketingCompanion(llm=llm_fast, temperature=0.8, max_loops=3,\n",
    "similarity_threshold=0.96)\n",
    "eng = BugTriageCompanion(llm=llm_deep, temperature=0.3)\n",
    "plan = StrategyCompanion(llm=llm_fast)\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "  input: str\n",
    "  marketing: str\n",
    "  engineering: str\n",
    "  merged: str\n",
    "  final_plan: str\n",
    "\n",
    "graph = StateGraph(GraphState)\n",
    "graph.add_node(\"marketing_agent\", lambda state: {\"marketing\": mkt(state[\"input\"])})\n",
    "graph.add_node(\"engineering_agent\", lambda state: {\"engineering\": eng(state[\"input\"])})\n",
    "graph.add_node(\"merge_agent\", lambda state: {\"merged\": \n",
    "                f\"### Marketing\\n{state['marketing']}\\n\\n### Engineering\\n{state['engineering']}\"\n",
    "                })\n",
    "graph.add_node(\"strategy_agent\", lambda state: {\"final_plan\": plan(state[\"merged\"])})\n",
    "\n",
    "# SEQUENTIAL execution - marketing first, then engineering\n",
    "graph.add_edge(\"__start__\", \"marketing_agent\")\n",
    "graph.add_edge(\"marketing_agent\", \"engineering_agent\")  # ← Key difference: sequential\n",
    "graph.add_edge(\"engineering_agent\", \"merge_agent\")\n",
    "graph.add_edge(\"merge_agent\", \"strategy_agent\")\n",
    "graph.set_finish_point(\"strategy_agent\")\n",
    "\n",
    "workflow = graph.compile()\n",
    "print(\"Graph compiled successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a241892-f833-47f7-a928-ed459e89e592",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the graph\n",
    "display(Image(workflow.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412d79c8-9c31-4ebf-8828-3ce4656fec33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing sequential execution with streaming...\\n\")\n",
    "for chunk in workflow.stream(\n",
    "  {\"input\": \"App ratings fell to 3.2★ and uploads crash on iOS 17.2. Diagnose & propose next steps.\"},\n",
    "  stream_mode=\"values\"\n",
    "):\n",
    "  print(chunk)\n",
    "  print(\"\\n---\\n\")\n",
    "\n",
    "print(\"✓ Sequential execution completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb86558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# different models for differents domains\n",
    "llm_fast  = \"gpt-5-mini\"\n",
    "llm_deep  = \"gpt-5\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7e0931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the agents\n",
    "# tip read the doctring by hovering over the class\n",
    "mkt   = MarketingCompanion(llm=llm_fast, temperature=0.8, max_loops=3, similarity_threshold=0.96)\n",
    "eng   = BugTriageCompanion(llm=llm_deep, temperature=0.3)\n",
    "plan = StrategyCompanion(llm=llm_fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a0e857-eab0-4de1-987b-95c545175a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(mkt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463b02e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RA companions work as drop-in LangGraph nodes!\n",
    "# The __call__ method makes them compatible with RunnableLambda\n",
    "# No special integration needed - they just work together\n",
    "\n",
    "#mkt_node  = RunnableLambda(mkt)          # Marketing companion → LangGraph node\n",
    "#eng_node  = RunnableLambda(eng)          # Engineering companion → LangGraph node\n",
    "\n",
    "# Now these nodes have BOTH:\n",
    "# - LangGraph's orchestration capabilities (streaming, retries, etc.)\n",
    "# - RA's thinking transparency (critique/revision history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cb39cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge-lambda joins text views into one string\n",
    "# note: LangGraph passes the entire upstream-state dict to a node.\n",
    "# with out this function, two upstream nodes are piped straight into strategy, \n",
    "# so plan_node will receive a Python dict like {\"engineering\": \"...\", \"marketing\": \"...\"}.\n",
    "# That's fine if your StrategyCompanion prompt expects that JSON blob, \n",
    "# but most of the time you'll want to concatenate the two strings first.\n",
    "\n",
    "#merge_node = RunnableLambda(\n",
    "    #lambda d: f\"### Marketing\\n{d['marketing']}\\n\\n### Engineering\\n{d['engineering']}\"\n",
    "#)\n",
    "\n",
    "\n",
    "#plan_node  = RunnableLambda(plan)\n",
    "\n",
    "# Define the state schema for LangGraph\n",
    "class GraphState(TypedDict):\n",
    "    input: str\n",
    "    marketing: str\n",
    "    engineering: str\n",
    "    merged: str\n",
    "    final_plan: str\n",
    "\n",
    "# Inline LangGraph example (fan-in)\n",
    "# No extra prompts, no schema gymnastics: simply passing text between the callables the classes already expose.\n",
    "graph = StateGraph(GraphState)\n",
    "graph.add_node(\"marketing_agent\",    lambda state: {\"marketing\": mkt(state[\"input\"])})\n",
    "graph.add_node(\"engineering_agent\",  lambda state: {\"engineering\": eng(state[\"input\"])})\n",
    "graph.add_node(\"merge_agent\",        lambda state: {\n",
    "    \"merged\": f\"### Marketing\\n{state['marketing']}\\n\\n### Engineering\\n{state['engineering']}\"\n",
    "    })\n",
    "graph.add_node(\"strategy_agent\",     lambda state: {\"final_plan\": plan(state[\"merged\"])})\n",
    "\n",
    "graph.add_edge(\"marketing_agent\", \"merge_agent\")\n",
    "graph.add_edge(\"engineering_agent\", \"merge_agent\")\n",
    "graph.add_edge(\"merge_agent\", \"strategy_agent\")\n",
    "\n",
    "graph.add_edge(\"__start__\", \"marketing_agent\")\n",
    "graph.add_edge(\"__start__\", \"engineering_agent\")\n",
    "graph.set_finish_point(\"strategy_agent\")\n",
    "workflow = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e326c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the graph\n",
    "display(Image(workflow.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9rksmscmbsi",
   "metadata": {},
   "source": [
    "## Part 1: LangGraph Debug Stream\n",
    "\n",
    "### Understanding LangGraph's Structured Debug Data\n",
    "\n",
    "LangGraph provides workflow-level debugging through its debug stream. Let's run the workflow with `stream_mode=\"debug\"` to capture structured debug events.\n",
    "\n",
    "**What you'll see**: Task scheduling, execution order, and results - useful for understanding workflow orchestration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258ed155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture debug stream data with stream_mode=\"debug\"\n",
    "debug_chunks = []\n",
    "print(\"Running workflow with debug mode enabled...\\n\")\n",
    "\n",
    "# Note: stream_mode=\"debug\" returns structured debug data\n",
    "for chunk in workflow.stream(\n",
    "    {\"input\": \"App ratings fell to 3.2★ and uploads crash on iOS 17.2. Diagnose & propose next steps.\"},\n",
    "    stream_mode=\"debug\"\n",
    "):\n",
    "    debug_chunks.append(chunk)  # Now we can capture structured data!\n",
    "    print(chunk)  # Also print for visibility\n",
    "    print(\"\\n\")\n",
    "print(\"✓ Parallel execution completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y31qd87f2r",
   "metadata": {},
   "source": [
    "### Debug Output Analysis\n",
    "\n",
    "Now let's see what we captured in our debug chunks:\n",
    "\n",
    "```python\n",
    "print(f\"Total debug chunks captured: {len(debug_chunks)}\")\n",
    "print(f\"Chunk types: {set(chunk.get('type') for chunk in debug_chunks if 'type' in chunk)}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5yiqfitxnwd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze what we capturedprint(\"Total debug chunks captured: {len(debug_chunks)}\")\n",
    "print(f\"Chunk types: {set(chunk.get('type') for chunk in debug_chunks if 'type' in chunk)}\")\n",
    "\n",
    "# Extract task results\n",
    "task_results = [chunk for chunk in debug_chunks if chunk.get('type') == 'task_result']\n",
    "print(f\"\\nTask results found: {len(task_results)}\")\n",
    "for result in task_results:\n",
    "    print(f\"  - {result.get('payload', {}).get('name', 'unknown')}\")\n",
    "\n",
    "# Add this to print the actual text\n",
    "payload = result.get('payload', {})\n",
    "if 'result' in payload and payload['result']:\n",
    "    text = payload['result'][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df83be57-f427-44df-b412-02b283eecdac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "wk7h35tj92",
   "metadata": {},
   "source": [
    "### Extracting Results from Debug Chunks\n",
    "\n",
    "Let's extract the actual agent outputs from the debug stream:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vatkq28jen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results from debug chunks\n",
    "extracted_results = {}\n",
    "\n",
    "for chunk in debug_chunks:\n",
    "    if chunk.get('type') == 'task_result':\n",
    "        payload = chunk.get('payload', {})\n",
    "        agent_name = payload.get('name', 'unknown')\n",
    "        \n",
    "        # Navigate the nested structure to get the result\n",
    "        if 'result' in payload and payload['result']:\n",
    "            # Result is in format: [(key, value)]\n",
    "            result_value = payload['result'][0][1]\n",
    "            extracted_results[agent_name] = result_value\n",
    "\n",
    "# Show what we extracted\n",
    "print(\"Extracted agent outputs from debug stream:\")\n",
    "for agent, output in extracted_results.items():\n",
    "    print(f\"\\n{agent}:\")\n",
    "    print(f\"{output[:200]}...\" if len(output) > 200 else output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c2dcba-a510-4146-afd9-e2ed3aea64df",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(extracted_results[\"merge_agent\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yfybxlnu3vq",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Accessing RA's Built-in Introspection\n",
    "\n",
    "#### Note: Running the compiled workflow again to show RA's automatic history tracking\n",
    "\n",
    "Now let's run the workflow normally and access the thinking history that RA agents automatically preserve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abe8583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the workflow again - this time WITHOUT debug mode\n",
    "# This is what you'd typically do in production\n",
    "result = workflow.invoke(\n",
    "    {\"input\": \"App ratings fell to 3.2★ and uploads crash on iOS 17.2. Diagnose & propose next steps.\"}\n",
    ")\n",
    "\n",
    "print(\"Workflow completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2831002",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = result.get(\"final_plan\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa5c4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== FINAL PLAN ===\\n\")\n",
    "display(Markdown(final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d89648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === After LangGraph workflow completes ===\n",
    "print(\"DEEP INTROSPECTION - What LangGraph can't normally show you - available without any special configuration::\\n\")\n",
    "# Show why each converged\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPLETE CONVERGENCE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for name, agent in [(\"Marketing\", mkt), (\"Engineering\", eng), (\"Strategy\", plan)]:\n",
    "    print(f\"\\n{name} Companion:\")\n",
    "    print(f\"  • Model: {agent.llm.model_name}\")\n",
    "    print(f\"  • Temperature: {agent.llm.temperature}\")\n",
    "    print(f\"  • Iterations: {len(agent.run_log)}/{agent.max_loops}\")\n",
    "    print(f\"  • Similarity threshold: {agent.similarity_threshold}\")\n",
    "    \n",
    "    # Determine convergence type\n",
    "    last_critique = agent.run_log[-1]['critique'].lower()\n",
    "    if \"no further improvements\" in last_critique or \"minimal revisions\" in last_critique:\n",
    "        convergence = \"Critique-based (no improvements needed)\"\n",
    "    elif len(agent.run_log) < agent.max_loops:\n",
    "        convergence = \"Similarity-based (threshold reached)\"\n",
    "    else:\n",
    "        convergence = \"Max iterations reached\"\n",
    "    print(f\"  • Convergence: {convergence}\")\n",
    "\n",
    "# Want to see the last critique? Just access it directly!\n",
    "print(\"\\n Strategy's final critique (no parsing needed) (first 300 chars):\")\n",
    "print(f\"{plan.run_log[-1]['critique'][:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06pe70pskm8h",
   "metadata": {},
   "source": [
    "### The Complete Thinking History\n",
    "\n",
    "Now let's see what the debug stream couldn't show us - the complete Draft → Critique → Revision cycles for each agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f670d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show full thinking process for each agent\n",
    "print(\"\\n MARKETING THINKING PROCESS:\")\n",
    "display(Markdown(mkt.transcript_as_markdown()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518a7737",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n ENGINEERING THINKING PROCESS:\")\n",
    "display(Markdown(eng.transcript_as_markdown()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f46b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n STRATEGY SYNTHESIS PROCESS:\")\n",
    "display(Markdown(plan.transcript_as_markdown()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5uyetstz0ux",
   "metadata": {},
   "source": [
    "For detailed analysis see [`docs/LangGraph_RA_comp.md`](../docs/LangGraph_RA_comp.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
